{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ## 数据源介绍\n"," 本次实验使用的是 UCI Machine Learning Repository 公开的 iris 鸢尾花数据\n","\n"," 其中数据区（data）含有样本数 150 ，特征 4 个：萼片长度（sepal length (cm)）、萼片宽度（sepal width (cm)）、花瓣长度（petal length (cm)）、花瓣宽度（petal width (cm)）\n","\n"," 花的类型（target）为 setosa、versicolor、virginica，分别标记为 0、1、2"]},{"cell_type":"markdown","metadata":{},"source":[" ## 1、初始化"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.datasets import load_iris\n","\n","iris = load_iris(as_frame=True)"]},{"cell_type":"markdown","metadata":{},"source":[" ## 2、平行坐标多维显示\n"," 由于每一朵花的实例都有 4 个属性，因此可以用平行坐标法的折线来表示\n","\n"," 可以发现，展示结果中有两种花的折线图走势非常相近，而第三种的差异较大"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pandas.plotting import parallel_coordinates\n","\n","parallel_coordinates(iris.frame,'target')\n","plt.title('raw data')\n","plt.show()\n","\n","std = iris.frame.iloc[:,:-1]\n","std = (std - std.min())/(std.max() - std.min())\n","std = pd.concat([std,pd.DataFrame(iris.frame['target'])],axis=1)\n","\n","parallel_coordinates(std,'target')\n","plt.title('Normalized data')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":[" ## 3、特征选择\n","\n"," 使用 PCA 提取 2 个特征并显示"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=2)\n","X = pca.fit_transform(iris.data.values)\n","dataPca = pd.DataFrame()\n","dataPca['X'] = X[:,0]\n","dataPca['Y'] = X[:,1]\n","\n","plt.scatter(dataPca['X'], dataPca['Y'])\n","plt.title('raw data')\n","plt.show()\n","print('pca ratio:', pca.explained_variance_ratio_)"]},{"cell_type":"markdown","metadata":{},"source":[" ## 4、K-Means聚类\n","\n"," 调用 sklearn 的 kmeans 方法对所有数据聚 3 类，并得到评价结果"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","kmeans = KMeans(n_clusters = 3)\n","kmeans.fit(iris.data.values, iris.target.values)\n","dataPca['kmeans'] = kmeans.predict(iris.data.values)\n","\n","\n","def classResultAdjust(array):\n","    temp=list(array)\n","    a=set(temp)\n","    mat=[[0,0,i] for i in range(len(a))]\n","    for i,j in enumerate(temp):\n","        mat[j][0] += i\n","        mat[j][1] += 1\n","    for i in range(len(a)):\n","        mat[i][0] /= mat[i][1]\n","    mat.sort()\n","    index = 0\n","    dic={}\n","    for i in mat:\n","        dic[i[2]]=index\n","        index += 1\n","    temp1 = [0 for i in range(len(temp))]\n","    for i,j in enumerate(temp):\n","        temp1[i] = dic[j]\n","    return temp1\n","dataPca['kmeans'] = classResultAdjust(dataPca['kmeans'])\n","\n","\n","plt.scatter(dataPca['X'], dataPca['Y'], c=iris.target.values)\n","plt.title(\"Truth\")\n","plt.show()\n","plt.scatter(dataPca['X'], dataPca['Y'], c=dataPca['kmeans'])\n","plt.title(\"KMeans result\")\n","plt.show()\n","\n","print('\\nKMeans confusion matrix:')\n","print(metrics.confusion_matrix(iris.target.values, dataPca['kmeans']))\n","print('\\nKMeans classification report:')\n","print(metrics.classification_report(iris.target.values, dataPca['kmeans'], target_names = iris.target_names))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5、机器学习分类\n","\n","与 kmeans 非监督聚类不同的是，MLModel 使用多元线性回归对 iris 数据集进行训练"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","class MLModel():\n","\n","    def __init__(self, dim, target_num):\n","        self.target_num = target_num\n","        self.w = np.zeros((dim, 1))\n","        self.b = 0.0\n","\n","    def forward(self, x):\n","        x = x @ self.w + self.b\n","        x = self.sigmoid(x)\n","        return x\n","\n","    def sigmoid(self, x):\n","        x = 1 / (1 + np.exp(-x))\n","        return x\n","\n","    def group(self, x):\n","        x = x.reshape(-1)\n","        x *= self.target_num\n","        x -= 0.5\n","        return np.round(x)\n","\n","    def train(self, x, y, lr):\n","        s = self.forward(x)\n","        loss = -np.mean(y * np.log(s) + (1 - y) * np.log(1 - s))\n","        dw = (x.T @ (s - y)) / x.shape[0]\n","        db = np.mean(s - y)\n","        self.w -= lr * dw\n","        self.b -= lr * db\n","        return loss\n","\n","    def fit(self, x_train, y_train, x_eval, y_eval, print_loss=False, epochs=100, lr=1):\n","        x_train_standard = (x_train - x_train.min()) / (x_train.max() - x_train.min())\n","        y_train = y_train.reshape(-1, 1)\n","        y_train_standard = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n","\n","        x_eval_standard = (x_eval - x_eval.min()) / (x_eval.max() - x_eval.min())\n","        y_eval = y_eval.reshape(-1, 1)\n","        y_eval_standard = (y_eval - y_eval.min()) / (y_eval.max() - y_eval.min())\n","\n","        for epoch in range(epochs):\n","            loss_train = self.train(x_train_standard, y_train_standard, lr)\n","            pred_train = self.predict(x_train)\n","            pred_eval = self.predict(x_eval)\n","            acc_train = accuracy_score(y_train, pred_train)\n","            acc_eval = accuracy_score(y_eval, pred_eval)\n","            if print_loss and (epoch + 1) % 10 == 0:\n","                print(f'epoch: {(epoch+1):03d} '+\n","                    f'loss_train: {loss_train:.4f} '+\n","                    f'acc_train: {acc_train:.4f} '+\n","                    f'acc_eval: {acc_eval:.4f}')\n","\n","    def predict(self, x: np.ndarray):\n","        x_flatten = (x - x.min()) / (x.max() - x.min())\n","        pred = self.group(self.forward(x_flatten))\n","        return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_eval_test_split(x, y, train_size, test_size):\n","    idx = np.arange(len(x))\n","    np.random.shuffle(idx)\n","    train_num = round(len(x) * train_size)\n","    test_num = round(len(x) * test_size)\n","    eval_num = len(x) - train_num - test_num\n","    x_train, y_train = x[idx[:train_num]], y[idx[:train_num]]\n","    x_eval, y_eval = x[idx[train_num: train_num + eval_num]], y[idx[train_num: train_num + eval_num]]\n","    x_test, y_test = x[idx[-test_num:]], y[idx[-test_num:]]\n","    return x_train, y_train, x_eval, y_eval, x_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_size = 0.4\n","test_size = 0.3\n","x_train, y_train, x_eval, y_eval, x_test, y_test = \\\n","    train_eval_test_split(iris.data.values, iris.target.values, train_size, test_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mlModel = MLModel(dim=iris.data.values.shape[1], target_num=len(iris.target_names))\n","mlModel.fit(x_train, y_train, x_eval, y_eval, print_loss=True, epochs=100, lr=1)\n","pred_test = mlModel.predict(x_test)\n","pca = PCA(n_components=2)\n","coo = pca.fit_transform(x_test)\n","\n","plt.scatter(coo[:, 0], coo[:, 1], c=y_test)\n","plt.title(\"Truth\")\n","plt.show()\n","plt.scatter(coo[:, 0], coo[:, 1], c=pred_test)\n","plt.title(\"ML result\")\n","plt.show()\n","\n","print(f'\\ntest size: {test_size}')\n","print(f'\\nacc_test: {accuracy_score(y_test, pred_test):.4f}')\n","print('\\nML confusion matrix:')\n","print(metrics.confusion_matrix(y_test, pred_test))\n","print('\\nML classification report:')\n","print(metrics.classification_report(y_test, pred_test, target_names = iris.target_names))"]}],"metadata":{"kernelspec":{"display_name":"custom","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"metadata":{"interpreter":{"hash":"b437b9a17f0a7da79e77e1db1b1b38bdbc72b3708d28b4095e7a2b2c99804f1b"}},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"c15e9a40f9e8b7d309c8eb4dedd0cfa48ff6b918f0b03a9e03396fe4887ada67"}}},"nbformat":4,"nbformat_minor":2}
